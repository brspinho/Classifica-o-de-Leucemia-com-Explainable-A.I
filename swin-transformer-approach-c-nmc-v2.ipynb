{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9384155,"sourceType":"datasetVersion","datasetId":5693373}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import gdown\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport timm\nimport torch\nimport torch.nn.functional as F\nimport cv2\n\nfrom collections import defaultdict\nfrom PIL import Image, ImageOps\nfrom tqdm import tqdm\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader, Dataset, WeightedRandomSampler, ConcatDataset, random_split\nfrom torchvision import models, transforms\nfrom torchvision.datasets import ImageFolder\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"# Plotar imagens aleatórias\n\ndef plot_random_images(dataset, gray=False):\n    random_idx = np.random.randint(0, len(dataset), 9)\n\n    plt.figure(figsize=(10, 10))\n    for i, img_index in enumerate(random_idx):\n        plt.subplot(3,3,i+1)\n        plt.grid(False)\n        image, label = dataset[img_index]\n        plt.title(label)\n        if gray:\n          plt.imshow(image.permute(1,2,0), cmap='gray')\n        else:\n          plt.imshow(image.permute(1,2,0))\n    \n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotar matriz de confusão\n\ndef plot_confusion_matrix_with_diagonal(\n    cm,\n    labels,\n    title_matrix='Matriz de Confusão Normalizada',\n    title_diagonal='Class Accuracies',\n    extra_diagonals=None,\n    extra_names=None\n):\n\n    row_sums = cm.sum(axis=1)\n\n    sorted_indices = np.argsort(row_sums)[::-1]\n\n    cm_sorted = cm[sorted_indices][:, sorted_indices]\n    labels_sorted = [labels[i] for i in sorted_indices]\n\n    cm_normalized = cm_sorted.astype('float') / cm_sorted.sum(axis=1)[:, np.newaxis]\n    diagonal = np.diag(cm_normalized)\n\n    accuracy_rows = [diagonal]\n    accuracy_labels = [title_diagonal]\n\n    if extra_diagonals is not None:\n        accuracy_rows += extra_diagonals\n    if extra_names is not None:\n        accuracy_labels += extra_names\n    elif extra_diagonals:\n        accuracy_labels += [f'Modelo {i+1}' for i in range(len(extra_diagonals))]\n\n    sns.set(font_scale=1.0)\n\n    fig = plt.figure(figsize=(14, 16))\n\n    ax1 = plt.subplot2grid((6, 1), (0, 0), rowspan=4)\n    sns.heatmap(cm_normalized, annot=np.round(cm_normalized, 2), fmt=\".2f\", cmap='flare',\n                xticklabels=labels_sorted, yticklabels=labels_sorted, linewidths=0.5,\n                cbar=True, annot_kws={\"size\": 8}, ax=ax1)\n    ax1.set_title(title_matrix)\n    ax1.set_xlabel('Previsão')\n    ax1.set_ylabel('Real')\n    ax1.tick_params(axis='x', rotation=90)\n    ax1.tick_params(axis='y', rotation=0)\n\n    ax2 = plt.subplot2grid((6, 1), (4, 0), rowspan=2)\n    sns.heatmap(accuracy_rows, annot=np.round(accuracy_rows, 2), fmt=\".2f\", cmap='flare',\n                xticklabels=labels_sorted, yticklabels=accuracy_labels, cbar=True,\n                linewidths=0.5, ax=ax2)\n    ax2.tick_params(axis='x', rotation=90)\n    ax2.tick_params(axis='y', rotation=0)\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Função auxiliar da train_classification()\n\ndef _train_step_classification(classifier_model, dataloader, criterion, optimizer, scheduler, device):\n    classifier_model.train()\n\n    total_loss = 0\n    all_preds = []\n    all_targets = []\n\n    for X, y in dataloader:\n        X, y = X.to(device), y.to(device)\n        y_pred = classifier_model(X)\n        loss = criterion(y_pred, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * X.size(0)\n\n        y_pred_labels = torch.argmax(y_pred, axis=1)\n        all_preds.extend(y_pred_labels.detach().cpu().numpy())\n        all_targets.extend(y.detach().cpu().numpy())\n\n    if scheduler:\n        scheduler.step()\n\n    mean_loss = total_loss / len(dataloader.dataset)\n    accuracy = accuracy_score(all_targets, all_preds)\n    precision = precision_score(all_targets, all_preds, average='binary', zero_division=0)\n    recall = recall_score(all_targets, all_preds, average='binary', zero_division=0)\n    f1 = f1_score(all_targets, all_preds, average='binary', zero_division=0)\n\n    return mean_loss, accuracy, precision, recall, f1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Função auxiliar da train_classification()\n\ndef _test_step_classification(classifier_model, dataloader, criterion, device):\n    classifier_model.eval()\n\n    total_loss = 0\n    all_preds = []\n    all_targets = []\n\n    with torch.inference_mode():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            y_pred = classifier_model(X)\n            loss = criterion(y_pred, y)\n\n            total_loss += loss.item() * X.size(0)\n\n            y_pred_labels = torch.argmax(y_pred, axis=1)\n            all_preds.extend(y_pred_labels.detach().cpu().numpy())\n            all_targets.extend(y.detach().cpu().numpy())\n\n    mean_loss = total_loss / len(dataloader.dataset)\n    accuracy = accuracy_score(all_targets, all_preds)\n    precision = precision_score(all_targets, all_preds, average='binary', zero_division=0)\n    recall = recall_score(all_targets, all_preds, average='binary', zero_division=0)\n    f1 = f1_score(all_targets, all_preds, average='binary', zero_division=0)\n\n    return mean_loss, accuracy, precision, recall, f1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Treina rede de classificação\n\ndef train_classification(classifier_model, train_dataloader, test_dataloader, criterion, epochs, optimizers, schedulers, device='cpu', verbose=True):\n    metrics = {'Train loss': [], 'Train accuracy': [], 'Train precision': [], 'Train recall': [], 'Train F1': [],\n               'Val loss': [], 'Val accuracy': [], 'Val precision': [], 'Val recall': [], 'Val F1': []}\n\n    current_optimizer_number = 0\n    for optimizer, scheduler, num_epochs in zip(optimizers, schedulers, epochs):\n        current_optimizer_number += 1\n        for epoch in tqdm(range(1, num_epochs+1), desc=f'Training with optimizer {current_optimizer_number}'):\n            current_lr = optimizer.param_groups[0][\"lr\"]\n            train_loss, train_accuracy, train_precision, train_recall, train_f1= _train_step_classification(classifier_model, train_dataloader, criterion, optimizer, scheduler, device)\n            test_loss, test_accuracy, test_precision, test_recall, test_f1 = _test_step_classification(classifier_model, test_dataloader, criterion, device)\n\n            metrics['Train loss'].append(train_loss)\n            metrics['Train accuracy'].append(train_accuracy)\n            metrics['Train precision'].append(train_precision)\n            metrics['Train recall'].append(train_recall)\n            metrics['Train F1'].append(train_f1)\n            metrics['Val loss'].append(test_loss)\n            metrics['Val accuracy'].append(test_accuracy)\n            metrics['Val precision'].append(test_precision)\n            metrics['Val recall'].append(test_recall)\n            metrics['Val F1'].append(test_f1)\n\n            if verbose:\n                print(f'\\nEPOCH {epoch} | Current learning rate: {current_lr:.8f}\\n'\n                      f'Train loss: {train_loss:.4f} | Train accuracy: {train_accuracy:.4f} | Train precision: {train_precision:.4f} | Train recall: {train_recall:.4f} | Train F1: {train_f1:.4f}\\n'\n                      f'Val loss: {test_loss:.4f} | Val accuracy: {test_accuracy:.4f} | Val precision: {test_precision:.4f} | Val recall: {test_recall:.4f} | Val F1: {test_f1:.4f}\\n')\n\n    return metrics","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Retorna matriz de confusão\n\ndef get_conf_matrix(classifier_model, dataloader, device):\n    classifier_model.eval()\n\n    all_preds = []\n    all_targets = []\n\n    with torch.inference_mode():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            y_pred = classifier_model(X).argmax(dim=1).cpu()\n\n            all_preds.extend(y_pred.cpu())\n            all_targets.extend(y.cpu())\n\n    conf_matrix = confusion_matrix(all_targets, all_preds)\n\n    return conf_matrix","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Wrapper pra receber o subset do dataset e aplicar o transform\n\nfrom torch.utils.data import Dataset\n\n# Wrapper que aplica transform no subset\nclass TransformedDataset(Dataset):\n    def __init__(self, subset, transform=None):\n        self.subset = subset   # pode ser random_split (Subset)\n        self.transform = transform\n\n    def __getitem__(self, idx):\n        x, y = self.subset[idx]   # retorna PIL Image + label\n        if self.transform:\n            x = self.transform(x)\n        return x, y\n\n    def __len__(self):\n        return len(self.subset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"train_transform = transforms.Compose([\n    transforms.CenterCrop(224),              \n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomVerticalFlip(p=0.5),        \n    transforms.RandomAffine(degrees=30, translate=(0.00, 0.00), scale=(1.00, 1.00), shear=5),\n    transforms.ToTensor(),\n    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nval_transform = transforms.Compose([\n    transforms.CenterCrop(224), \n    transforms.ToTensor(),\n    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntest_transform = transforms.Compose([\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fold_0 = ImageFolder(\"/kaggle/input/c-nmc-2019-dataset/C-NMC 2019 (PKG)/C-NMC_training_data/fold_0\")\nfold_1 = ImageFolder(\"/kaggle/input/c-nmc-2019-dataset/C-NMC 2019 (PKG)/C-NMC_training_data/fold_1\")\nfold_2 = ImageFolder(\"/kaggle/input/c-nmc-2019-dataset/C-NMC 2019 (PKG)/C-NMC_training_data/fold_2\")\n\ndataset = ConcatDataset([fold_0, fold_1, fold_2])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seed = 42\ng = torch.Generator().manual_seed(seed)\n\n# Tamanhos\nn_total = len(dataset)\nn_test = int(0.10 * n_total)\nn_val  = int(0.20 * n_total)\nn_train = n_total - n_val - n_test\n\n# Split\ntrain_dataset, val_dataset, test_dataset = random_split(\n    dataset, [n_train, n_val, n_test], generator=g\n)\n\nprint(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = TransformedDataset(train_dataset, transform=train_transform)\nval_dataset   = TransformedDataset(val_dataset,   transform=val_transform)\ntest_dataset  = TransformedDataset(test_dataset,  transform=test_transform)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Conta quantos exemplos existem de cada classe\ntargets = [y for _, y in train_dataset]\n\nclass_sample_counts = np.bincount(targets)  # [num_classe0, num_classe1]\nweights = 1. / class_sample_counts          # inverso da frequência\n\n# Cria vetor de pesos para cada exemplo\nsample_weights = [weights[t] for t in targets]\n\n# Sampler balanceado\nsampler = WeightedRandomSampler(\n    weights=sample_weights,\n    num_samples=30000,   # total de amostras desejadas\n    replacement=True     # permite repetir para balancear\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_random_images(train_dataset) #falta tirar a normalização depois, pra mostrar as imagens normalmente","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class SwinTransformerBase(nn.Module):\n    def __init__(self, drop_path_rate=0, drop_rate=0, attn_drop_rate=0):\n        super().__init__()\n        self.model = timm.create_model(\n            'swin_base_patch4_window7_224',\n            pretrained=True,\n            num_classes=2,\n            drop_path_rate=drop_path_rate,\n            drop_rate=drop_rate,\n            attn_drop_rate=attn_drop_rate\n        )\n\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Dispositivo (CPU ou GPU)\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=4, prefetch_factor=4, pin_memory=True, persistent_workers=True)\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, prefetch_factor=4, pin_memory=True, persistent_workers=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, prefetch_factor=4, pin_memory=True, persistent_workers=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Classification","metadata":{}},{"cell_type":"code","source":"from torch.optim.lr_scheduler import CosineAnnealingLR\n\nclassifier_model = SwinTransformerBase(drop_path_rate=0.2, drop_rate=0.1, attn_drop_rate=0.0).to(device)\ncriterion = nn.CrossEntropyLoss().to(device)\n\noptimizer = optim.AdamW(classifier_model.parameters(), lr=1e-4, weight_decay=5e-2) \n\n\nscheduler = optim.lr_scheduler.StepLR(optimizer, 1, 0.9)\n\noptimizers = [optimizer]\nschedulers = [scheduler]\nepochs = [50]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#plot_results(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Baixando parâmetros do drive (se necessário)\n#gdown.download('https://drive.google.com/file/d/1xowiBw8I_GhCwx2sTwM7ttyqrGkawpc7/view?usp=drive_link', 'parameters.pth', fuzzy=True, quiet=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Carregando parâmetros\n#classifier_model.load_state_dict(torch.load('/kaggle/working/parameters.pth'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Treino\nmetrics = train_classification(classifier_model, train_dataloader, val_dataloader, criterion, epochs, optimizers, schedulers, device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Avaliação Final no Conjunto de Teste ---\n\nprint(\"Avaliando o modelo final no conjunto de teste...\")\n\ntest_loss, test_accuracy, test_precision, test_recall, test_f1 = _test_step_classification(\n    classifier_model,\n    test_dataloader,\n    criterion,\n    device\n)\n\nprint(\"\\n--- Resultados Finais no Conjunto de Teste ---\")\nprint(f'Test loss: {test_loss:.4f}')\nprint(f'Test accuracy: {test_accuracy:.4f}')\nprint(f'Test precision: {test_precision:.4f}')\nprint(f'Test recall: {test_recall:.4f}')\nprint(f'Test F1: {test_f1:.4f}')\n\n\nprint(\"\\nGerando a Matriz de Confusão...\")\nconf_matrix = get_conf_matrix(classifier_model, test_dataloader, device)\n\nclass_names = fold_0.classes \n\nplot_confusion_matrix_with_diagonal(conf_matrix, labels=class_names, title_matrix='Matriz de Confusão (Conjunto de Teste)')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(classifier_model.state_dict(), 'parameters.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}